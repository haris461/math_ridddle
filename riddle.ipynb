{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "756f1fc5-cf29-43c9-a4a2-26c42b4f19f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 08:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.104600</td>\n",
       "      <td>2.149464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.781700</td>\n",
       "      <td>1.508261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.235100</td>\n",
       "      <td>1.305371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.092500</td>\n",
       "      <td>1.255582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.840900</td>\n",
       "      <td>1.215276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.748300</td>\n",
       "      <td>1.245532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>1.236750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>1.266319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.709400</td>\n",
       "      <td>1.291729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.567200</td>\n",
       "      <td>1.304875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./riddle-factory-model\\\\tokenizer_config.json',\n",
       " './riddle-factory-model\\\\special_tokens_map.json',\n",
       " './riddle-factory-model\\\\vocab.json',\n",
       " './riddle-factory-model\\\\merges.txt',\n",
       " './riddle-factory-model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load CSV File\n",
    "df = pd.read_csv(\"math_riddles.csv\")  # Use the uploaded CSV file path\n",
    "\n",
    "# Step 2: Convert CSV data to text format\n",
    "riddles_data = [f\"Riddle: {row['riddle']} Answer: {row['solution']}\" for _, row in df.iterrows()]\n",
    "\n",
    "# Step 3: Create a custom dataset\n",
    "class RiddlesDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, block_size=128):\n",
    "        self.examples = tokenizer(texts, truncation=True, padding=\"max_length\", \n",
    "                                  max_length=block_size, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)  # Ensure correct length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {\"input_ids\": self.examples[i], \"labels\": self.examples[i]}\n",
    "\n",
    "# Step 4: Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set pad_token_id to eos_token_id to avoid warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Step 5: Prepare dataset for fine-tuning\n",
    "dataset = RiddlesDataset(riddles_data, tokenizer)\n",
    "\n",
    "# Ensure the dataset is not empty\n",
    "assert len(dataset) > 0, \"Dataset is empty! Check CSV file and tokenization.\"\n",
    "\n",
    "# Step 6: Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% Training, 20% Validation\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Data collator for dynamic batching\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Step 7: Set up training arguments with validation tracking\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./riddle_factory\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,           \n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=5e-5,           \n",
    "    warmup_steps=10,              \n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,              \n",
    "    evaluation_strategy=\"epoch\",  # Evaluate validation loss at the end of each epoch\n",
    ")\n",
    "\n",
    "# Step 8: Initialize Trainer with training and validation datasets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  \n",
    ")\n",
    "\n",
    "# Step 9: Train and track validation loss\n",
    "trainer.train()\n",
    "\n",
    "# Step 10: Save the fine-tuned model\n",
    "model.save_pretrained(\"./riddle-factory-model\")\n",
    "tokenizer.save_pretrained(\"./riddle-factory-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "870d3780-9d8e-4b02-b834-ab285a205081",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2001919508.py, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[26], line 58\u001b[1;36m\u001b[0m\n\u001b[1;33m    mult = float(re.search r'multiply by (\\d+)', riddle_part).group(1))\u001b[0m\n\u001b[1;37m                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"./riddle-factory-model\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set pad_token_id to eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_riddle(prompt=\"Riddle:\", max_length=100, num_return_sequences=5):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,  \n",
    "        top_k=40,        \n",
    "        top_p=0.9,      \n",
    "        temperature=0.7, \n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    riddles_with_answers = [tokenizer.decode(riddle, skip_special_tokens=True) for riddle in output]\n",
    "    \n",
    "    for i, text in enumerate(riddles_with_answers, 1):\n",
    "        if \"Answer:\" in text:\n",
    "            riddle_part, answer_part = text.split(\"Answer:\", 1)\n",
    "            # Extract first numeric answer\n",
    "            numbers = re.findall(r'\\d+(?:\\.\\d+)?', answer_part.split('.')[0])\n",
    "            answer = numbers[0] if numbers else answer_part.split('.')[0].strip()\n",
    "            print(f\"Riddle {i}: {riddle_part.strip()}\")\n",
    "            print(f\"Generated Answer: {answer}\")\n",
    "            # Basic validation\n",
    "            try:\n",
    "                x = float(answer)\n",
    "                if \"subtract\" in riddle_part and \"twice\" in riddle_part:\n",
    "                    num = float(re.search(r'twice (\\d+)', riddle_part).group(1))\n",
    "                    eq = float(re.search(r'get (\\d+)', riddle_part).group(1))\n",
    "                    sub = float(re.search(r'subtract (\\d+)', riddle_part).group(1))\n",
    "                    correct = x - sub == 2 * num\n",
    "                    print(f\"Correct? {correct} (Expected: {2 * num + sub})\")\n",
    "                elif \"multiply\" in riddle_part and \"subtract\" in riddle_part:\n",
    "                    mult = float(re.search(r'multiply me by (\\d+)', riddle_part).group(1))\n",
    "                    sub = float(re.search(r'subtract (\\d+)', riddle_part).group(1))\n",
    "                    eq = float(re.search(r'get (\\d+)', riddle_part).group(1))\n",
    "                    correct = mult * x - sub == eq\n",
    "                    print(f\"Correct? {correct} (Expected: {(eq + sub) / mult})\")\n",
    "                elif \"add\" in riddle_part and \"multiply\" in riddle_part:\n",
    "                    add = float(re.search(r'add (\\d+)', riddle_part).group(1))\n",
    "                    mult = float(re.search(r'multiply by (\\d+)', riddle_part).group(1))  # Fixed syntax\n",
    "                    eq = float(re.search(r'get (\\d+)', riddle_part).group(1))\n",
    "                    correct = mult * (x + add) == eq\n",
    "                    print(f\"Correct? {correct} (Expected: {(eq / mult) - add})\")\n",
    "                else:\n",
    "                    print(\"Correct? Requires manual check\")\n",
    "            except:\n",
    "                print(\"Correct? Requires manual check\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"Riddle {i}: {text.strip()}\\nAnswer: Not generated.\\n\")\n",
    "\n",
    "# Generate and display riddles with answers\n",
    "generate_riddle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725064ed-ba12-4550-a004-da6a30b2cc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.621400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Riddles:\n",
      "1. Riddle: Â How do you get a new key to unlock the \"Vampire's\" key? Answer: The key is a number. How do I get the number?Â  Answer Answer : I add 12 to the key.Â  If I have 12, I am able to get my number by adding 6 to my key (or 9 if I do not add 6).Â  I then add my other number to it. Answer 2: I subtract the first number from me, and add\n",
      "2. Riddle: ?\"\n",
      "\n",
      "Answer: \"Yes.\"\n",
      ".\n",
      "-\n",
      "Riddles: 1-2: What if I was a human?\n",
      "Jade: Answer: 3. Answer? \"What if you were a man?\" Answer 3: I am a woman? Answer 4: No, I'm a Human? I can't be a Woman? (Answer 4)\n",
      "Question: 4-9: When I die, am I a God? Answers: 5, 8,\n",
      "3. Riddle: Â How do you get from one point to another? Answer: Answer Number 1. _________________________________________________________________________\n",
      "Question: How do I get to a point by one? ________Answer: I am now one; I will now get there by two.\n",
      "Answer, number 3: Now, I'll get me to one by multiplying by 2; then I'm now two, and I can get back to number 1 by four?\n",
      "_____________________________________________________________________\n",
      "A question: What if\n",
      "4. Riddle: ????\n",
      "Q: What is the number?\n",
      "A: 7\n",
      "And what number is it? Answer: 5\n",
      "The number of the house of three equals three.\n",
      "I am not a number. What number am I? I am a house number, but I'm not seven? Is it a million? What am my number when I have two houses? A house equals two? And I live in the same house as a man? If I had three houses, what\n",
      "5. Riddle: Â What is a black hole? Answer: A black circle with a number in the center, and a zero in its center. Answer is:Â  1,000. Note: The number is only half of the number.\n",
      "Q: What is the value of an equation? Â  Answer, 1. If you add 2, you get 3. Add 1 + 3, then you subtract 2. You add 6, add 12, etc. ???? ????? ??? \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bcc10-52de-4cb9-bf5d-d0550b06067d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
